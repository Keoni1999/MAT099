{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Arima"
      ],
      "metadata": {
        "id": "Y4UPh0-fLMeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Patent data and GDP-related data reloaded\n",
        "patent_data = pd.read_csv('/content/Lv4Nissan1980To2010.csv')\n",
        "gdp_data = pd.read_csv('/content/gdp_data.csv')\n",
        "\n",
        "# Show the first few rows of both datasets\n",
        "patent_data.head(), gdp_data.head()\n"
      ],
      "metadata": {
        "id": "mnsgo9J6LHa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check patent data for missing and duplicate values\n",
        "patent_missing_values = patent_data.isnull().sum().sum()\n",
        "patent_duplicate_rows = patent_data.duplicated().sum()\n",
        "\n",
        "# Check for missing and duplicate values in GDP-related data\n",
        "gdp_missing_values = gdp_data.isnull().sum().sum()\n",
        "gdp_duplicate_rows = gdp_data.duplicated().sum()\n",
        "\n",
        "patent_missing_values, patent_duplicate_rows, gdp_missing_values, gdp_duplicate_rows\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr2pfYQsLIkw",
        "outputId": "2fb17a3a-85d7-4293-84b7-4491d7d4428c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate descriptive statistics of patent data\n",
        "patent_data_description = patent_data.describe()\n",
        "\n",
        "# Calculation of descriptive statistics for GDP-related data (numerical columns only)\n",
        "gdp_data_description = gdp_data.describe()\n",
        "\n",
        "patent_data_description, gdp_data_description\n"
      ],
      "metadata": {
        "id": "7pFJArGSLJ_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total annual number of patents\n",
        "annual_patent_sum = patent_data.sum(numeric_only=True)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "annual_patent_sum.plot()\n",
        "plt.title(\"Nissan Patent Number Trend Chart - 1980-2015\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of patents\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QuxylrUqLO-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View column names for GDP data\n",
        "gdp_data.columns"
      ],
      "metadata": {
        "id": "ABbw69NlLRAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer of GDP data\n",
        "gdp_data_transposed = gdp_data.set_index(\"year\").transpose().reset_index()\n",
        "gdp_data_transposed = gdp_data_transposed.rename(columns={\"index\": \"Year\"})\n",
        "\n",
        "# Charting trends in GDP-related indicators\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "for column in gdp_data_transposed.columns[1:]:\n",
        "    plt.plot(gdp_data_transposed[\"Year\"], gdp_data_transposed[column], label=column)\n",
        "\n",
        "plt.title(\"Trend charts for GDP-related indicators\")\n",
        "plt.xlabel(\"year\")\n",
        "plt.ylabel(\"metric\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PZWLgrtcLTHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number for each patent category\n",
        "patent_category_sum = patent_data.set_index(\"Unnamed: 0\").sum(axis=1)\n",
        "\n",
        "# Select the top 10 patent categories with the most total counts\n",
        "top_patent_categories = patent_category_sum.nlargest(10)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "top_patent_categories.plot(kind='bar')\n",
        "plt.title(\"Top 10 Patent Categories by Total Counts\")\n",
        "plt.xlabel(\"Types of Patents\")\n",
        "plt.ylabel(\"Number of patents\")\n",
        "plt.grid(axis='y')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tITQQ9jzLUVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform time series decomposition\n",
        "result = seasonal_decompose(annual_patent_sum, model='additive', period=1)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(3, 1, 1)\n",
        "result.trend.plot(title='Nissan Patent Number Trend Chart - 1980-2015')\n",
        "plt.subplot(3, 1, 2)\n",
        "result.seasonal.plot(title='Seasonal')\n",
        "plt.subplot(3, 1, 3)\n",
        "result.resid.plot(title='Residual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IN76csvYLVse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data type of the \"Year\" column in gdp_data_transposed\n",
        "gdp_data_transposed[\"Year\"].dtype\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-K4dt6ZLW_2",
        "outputId": "f79912ad-f19b-4fa7-f301-819f3faa15e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('O')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the \"Year\" column in gdp_data_transposed to integer type\n",
        "gdp_data_transposed[\"Year\"] = gdp_data_transposed[\"Year\"].astype(int)\n"
      ],
      "metadata": {
        "id": "havNfmYKLYNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since annual_patent_sum is a Series, we first need to convert it to a DataFrame and rename its index column to \"Year\"\n",
        "annual_patent_df = annual_patent_sum.reset_index()\n",
        "annual_patent_df.columns = [\"Year\", \"Total_Patents\"]\n",
        "annual_patent_df[\"Year\"] = annual_patent_df[\"Year\"].astype(int)  # Convert to integer type\n"
      ],
      "metadata": {
        "id": "DT4jLnlwLZXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "combined_data_updated = pd.merge(annual_patent_df, gdp_data_transposed, on=\"Year\", how=\"left\")\n",
        "combined_data_updated.head()"
      ],
      "metadata": {
        "id": "dEqtjMjELaXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the trend line chart of patent counts versus various economic indicators\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "# Plot the trend of patent counts\n",
        "plt.plot(combined_data_updated[\"Year\"], combined_data_updated[\"Total_Patents\"], label=\"Total Patents\", color=\"black\", linewidth=2)\n",
        "\n",
        "# Plot the trends of various economic indicators\n",
        "for column in combined_data_updated.columns[2:]:\n",
        "    plt.plot(combined_data_updated[\"Year\"], combined_data_updated[column], label=column)\n",
        "\n",
        "plt.title(\"Trend Line Chart of Patent Counts vs. Various Economic Indicators\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Jt4Z8LrlLdsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation of patent counts between years\n",
        "patent_correlation_matrix = patent_data.drop(columns=[\"Unnamed: 0\"]).corr()\n",
        "\n",
        "# Display the correlation of patent counts using a heatmap\n",
        "plt.figure(figsize=(18, 14))\n",
        "plt.title('Heat map of correlation of number of patents between years')\n",
        "sns.heatmap(patent_correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=0.5)\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "edTy427QLeU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the box plot of patent data\n",
        "plt.figure(figsize=(16, 6))\n",
        "sns.boxplot(data=patent_data.drop(columns=[\"Unnamed: 0\"]))\n",
        "plt.title(\"Box Plot of Patent Counts from 1980 to 2015\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of Patents\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot the violin plot of patent data\n",
        "plt.figure(figsize=(16, 6))\n",
        "sns.violinplot(data=patent_data.drop(columns=[\"Unnamed: 0\"]))\n",
        "plt.title(\"Violin Plot of Patent Counts from 1980 to 2015\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of Patents\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xO5MakumLf0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the box plot of economic indicator data\n",
        "plt.figure(figsize=(16, 6))\n",
        "sns.boxplot(data=gdp_data_transposed.drop(columns=[\"Year\"]))\n",
        "plt.title(\"Box Plot of Economic Indicator Data from 1980 to 2015\")\n",
        "plt.xlabel(\"Economic Indicators\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot the violin plot of economic indicator data\n",
        "plt.figure(figsize=(16, 6))\n",
        "sns.violinplot(data=gdp_data_transposed.drop(columns=[\"Year\"]))\n",
        "plt.title(\"Violin Plot of Economic Indicator Data from 1980 to 2015\")\n",
        "plt.xlabel(\"Economic Indicators\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ECHSaciaLhcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the IQR method to detect outliers\n",
        "def detect_outliers_iqr(series):\n",
        "    \"\"\"\n",
        "    Detect outliers using the IQR method\n",
        "    \"\"\"\n",
        "    Q1 = series.quantile(0.25)\n",
        "    Q3 = series.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    return series[(series < lower_bound) | (series > upper_bound)]\n",
        "\n",
        "# Detect outliers for each year in the patent data\n",
        "patent_outliers = {}\n",
        "for column in patent_data.columns[1:]:\n",
        "    patent_outliers[column] = detect_outliers_iqr(patent_data[column])\n",
        "\n",
        "patent_outliers_df = pd.DataFrame.from_dict(patent_outliers, orient='index').transpose()\n",
        "\n",
        "patent_outliers_df\n"
      ],
      "metadata": {
        "id": "30NeqLgILkFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect outliers for each year in the economic indicator data\n",
        "gdp_outliers = {}\n",
        "for column in gdp_data_transposed.columns[1:]:\n",
        "    gdp_outliers[column] = detect_outliers_iqr(gdp_data_transposed[column])\n",
        "\n",
        "gdp_outliers_df = pd.DataFrame.from_dict(gdp_outliers, orient='index').transpose()\n",
        "\n",
        "gdp_outliers_df\n"
      ],
      "metadata": {
        "id": "AKVroY-PLmmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove patent categories from patent data where the patent count is 0 for all years\n",
        "filtered_patent_data = patent_data[patent_data.drop(columns=[\"Unnamed: 0\"]).sum(axis=1) > 0]\n",
        "\n",
        "# Define a function to replace outliers\n",
        "def replace_outliers_with_median(series):\n",
        "    \"\"\"\n",
        "    Replace outliers with the median\n",
        "    \"\"\"\n",
        "    Q1 = series.quantile(0.25)\n",
        "    Q3 = series.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers_indices = series[(series < lower_bound) | (series > upper_bound)].index\n",
        "    series[outliers_indices] = series.median()\n",
        "\n",
        "    return series\n",
        "\n",
        "# Handle outliers for each year in the filtered patent data\n",
        "for column in filtered_patent_data.columns[1:]:\n",
        "    filtered_patent_data[column] = replace_outliers_with_median(filtered_patent_data[column])\n",
        "\n",
        "# Handle outliers for each year in the economic indicator data\n",
        "for column in gdp_data_transposed.columns[1:]:\n",
        "    gdp_data_transposed[column] = replace_outliers_with_median(gdp_data_transposed[column])\n",
        "\n",
        "filtered_patent_data, gdp_data_transposed\n"
      ],
      "metadata": {
        "id": "yOgOGLCkLnq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose the patent data to match the format\n",
        "filtered_patent_data_transposed = filtered_patent_data.set_index(\"Unnamed: 0\").transpose().reset_index()\n",
        "filtered_patent_data_transposed.rename(columns={\"index\": \"Year\"}, inplace=True)\n",
        "filtered_patent_data_transposed[\"Year\"] = filtered_patent_data_transposed[\"Year\"].astype(int)\n",
        "\n",
        "# Merge the economic indicator data with the transposed patent data\n",
        "merged_data = filtered_patent_data_transposed.merge(gdp_data_transposed, on=\"Year\", how=\"left\")\n",
        "\n",
        "merged_data\n",
        "# Save the merged data to a CSV file\n",
        "merged_data.to_csv('merged_patent_gdp_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Ff0qtJ6eLq1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose the data\n",
        "melted_data = merged_data.melt(id_vars=[\"Year\"], var_name=\"Feature\", value_name=\"Value\")\n",
        "melted_data[\"Year\"] = melted_data[\"Year\"].astype(int)  # Convert the 'Year' column to integer type\n",
        "\n",
        "melted_data.head()\n"
      ],
      "metadata": {
        "id": "3-yBDMllLsUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_data = melted_data[melted_data[\"Year\"] <= 2010]\n",
        "test_data = melted_data[melted_data[\"Year\"] > 2010]\n",
        "\n",
        "train_data.shape, test_data.shape\n"
      ],
      "metadata": {
        "id": "LWt094vdLt7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add lagged terms\n",
        "train_data[\"Lag1\"] = train_data.groupby(\"Feature\")[\"Value\"].shift(1)\n",
        "test_data[\"Lag1\"] = test_data.groupby(\"Feature\")[\"Value\"].shift(1)\n",
        "\n",
        "# For the first year of the training data, the lagged term will be NaN, so we set it to 0\n",
        "train_data[\"Lag1\"].fillna(0, inplace=True)\n",
        "\n",
        "train_data.head()\n"
      ],
      "metadata": {
        "id": "MmIwsCblLvYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add rolling window statistical features\n",
        "train_data[\"Rolling_Mean_3\"] = train_data.groupby(\"Feature\")[\"Value\"].transform(lambda x: x.rolling(3).mean().shift(1))\n",
        "test_data[\"Rolling_Mean_3\"] = test_data.groupby(\"Feature\")[\"Value\"].transform(lambda x: x.rolling(3).mean().shift(1))\n",
        "\n",
        "# For the first two years of the training data, the rolling window statistical feature will be NaN, so we set it to 0\n",
        "train_data[\"Rolling_Mean_3\"].fillna(0, inplace=True)\n",
        "\n",
        "train_data.head()\n"
      ],
      "metadata": {
        "id": "e0lLg1u6LxQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "train_data[\"Feature_Encoded\"] = label_encoder.fit_transform(train_data[\"Feature\"])\n",
        "test_data[\"Feature_Encoded\"] = label_encoder.transform(test_data[\"Feature\"])\n",
        "\n",
        "train_data.head()\n"
      ],
      "metadata": {
        "id": "ekKHVqVELy3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "mInDPVpRL0ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_arima_parameters(train_series, p_max=3, d_max=2, q_max=3):\n",
        "    \"\"\"Automatically select ARIMA parameters\"\"\"\n",
        "    best_aic = float('inf')\n",
        "    best_order = None\n",
        "\n",
        "    for p in range(p_max + 1):\n",
        "        for d in range(d_max + 1):\n",
        "            for q in range(q_max + 1):\n",
        "                try:\n",
        "                    model = ARIMA(train_series, order=(p,d,q))\n",
        "                    model_fit = model.fit()\n",
        "                    if model_fit.aic < best_aic:\n",
        "                        best_aic = model_fit.aic\n",
        "                        best_order = (p, d, q)\n",
        "                except:\n",
        "                    continue\n",
        "    return best_order\n"
      ],
      "metadata": {
        "id": "yfAh8_y0L1hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Use the function above to re-simulate the model and make predictions\n",
        "maes = []\n",
        "mses = []\n",
        "r2_scores = []\n",
        "predictions_dict = {}\n",
        "\n",
        "for feature in train_data[\"Feature\"].unique():\n",
        "    train_series = train_data[train_data[\"Feature\"] == feature].set_index(\"Year\")[\"Value\"]\n",
        "    test_series = test_data[test_data[\"Feature\"] == feature].set_index(\"Year\")[\"Value\"]\n",
        "\n",
        "    # Get the best parameters\n",
        "    best_order = auto_arima_parameters(train_series)\n",
        "\n",
        "    # Make predictions using the best parameters\n",
        "    model = ARIMA(train_series, order=best_order)\n",
        "    model_fit = model.fit()\n",
        "    predictions = model_fit.forecast(steps=len(test_series))\n",
        "\n",
        "    # Store the predictions\n",
        "    predictions_dict[feature] = predictions\n",
        "\n",
        "    # Calculate MAE, MSE, and R^2\n",
        "    maes.append(mean_absolute_error(test_series, predictions))\n",
        "    mses.append(mean_squared_error(test_series, predictions))\n",
        "    r2_scores.append(r2_score(test_series, predictions))\n",
        "\n",
        "# Calculate the overall average MAE, MSE, and R^2\n",
        "average_mae = sum(maes) / len(maes)\n",
        "average_mse = sum(mses) / len(mses)\n",
        "average_rmse = (average_mse)**0.5\n",
        "average_r2 = sum(r2_scores) / len(r2_scores)\n",
        "\n",
        "# Convert the predictions dictionary to DataFrame and save it as a CSV file\n",
        "predictions_df = pd.DataFrame(predictions_dict)\n",
        "predictions_filepath = \"/content/predictions_merged.csv\"\n",
        "predictions_df.to_csv(predictions_filepath)\n"
      ],
      "metadata": {
        "id": "0EC36G5HL11B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Use the function above to re-simulate the model and make predictions\n",
        "maes = []\n",
        "mses = []\n",
        "r2_scores = []\n",
        "predictions_dict = {}\n",
        "\n",
        "for feature in train_data[\"Feature\"].unique():\n",
        "    train_series = train_data[train_data[\"Feature\"] == feature].set_index(\"Year\")[\"Value\"]\n",
        "    test_series = test_data[test_data[\"Feature\"] == feature].set_index(\"Year\")[\"Value\"]\n",
        "\n",
        "    # Get the best parameters\n",
        "    best_order = auto_arima_parameters(train_series)\n",
        "\n",
        "    # Make predictions using the best parameters\n",
        "    model = ARIMA(train_series, order=best_order)\n",
        "    model_fit = model.fit()\n",
        "    predictions = model_fit.forecast(steps=len(test_series))\n",
        "\n",
        "    # Store the predictions\n",
        "    predictions_dict[feature] = predictions\n",
        "\n",
        "    # Calculate MAE, MSE, and R^2\n",
        "    maes.append(mean_absolute_error(test_series, predictions))\n",
        "    mses.append(mean_squared_error(test_series, predictions))\n",
        "    r2_scores.append(r2_score(test_series, predictions))\n",
        "\n",
        "# Calculate the overall average MAE, MSE, and R^2\n",
        "average_mae = sum(maes) / len(maes)\n",
        "average_mse = sum(mses) / len(mses)\n",
        "average_rmse = (average_mse)**0.5\n",
        "average_r2 = sum(r2_scores) / len(r2_scores)\n",
        "\n",
        "# Convert the predictions dictionary to DataFrame and save it as a CSV file\n",
        "predictions_df = pd.DataFrame(predictions_dict)\n",
        "predictions_filepath = \"predictions_merged2.csv\"\n",
        "predictions_df.to_csv(predictions_filepath)\n"
      ],
      "metadata": {
        "id": "hI6rGET1L509"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Load the actual values\n",
        "true_values_df = pd.read_csv('/content/merged_patent_gdp_data.csv')\n",
        "# Select the data from 2011 to 2015\n",
        "true_values_df = true_values_df[true_values_df['Year'].between(2011, 2015)]\n",
        "\n",
        "# Load the predicted values\n",
        "predictions_df = pd.read_csv('/content/predictions_merged.csv')\n",
        "# Rename the first column to \"Year\"\n",
        "predictions_df.rename(columns={predictions_df.columns[0]: \"Year\"}, inplace=True)\n",
        "predictions_df.head()\n",
        "# Replace the years in the predictions to 2011-2015\n",
        "predictions_df['Year'] = predictions_df['Year'].replace({31: 2011, 32: 2012, 33: 2013, 34: 2014, 35: 2015})\n"
      ],
      "metadata": {
        "id": "dmgNZ55tL6bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of true_values:\", true_values_df.shape)\n",
        "print(\"Shape of predictions:\", predictions_df.shape)\n",
        "\n",
        "print(\"\\nHead of true_values:\")\n",
        "print(true_values_df.head())\n",
        "\n",
        "print(\"\\nHead of predictions:\")\n",
        "print(predictions_df.head())"
      ],
      "metadata": {
        "id": "dbh6h78aL-ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Load the actual values\n",
        "true_values_df = pd.read_csv('/content/merged_patent_gdp_data.csv')\n",
        "# Select the data from 2011 to 2015\n",
        "true_values_df = true_values_df[true_values_df['Year'].between(2011, 2015)]\n",
        "\n",
        "# Load the predicted values\n",
        "predictions_df = pd.read_csv('/content/predictions_merged.csv')\n",
        "# Rename the first column to \"Year\"\n",
        "predictions_df.rename(columns={predictions_df.columns[0]: \"Year\"}, inplace=True)\n",
        "predictions_df.head()\n",
        "# Replace the years in the predictions to 2011-2015\n",
        "predictions_df['Year'] = predictions_df['Year'].replace({31: 2011, 32: 2012, 33: 2013, 34: 2014, 35: 2015})\n",
        "\n",
        "# Use a regular expression to select columns related to patents\n",
        "patent_columns = true_values_df.filter(regex='^[A-Z0-9]+$').columns\n",
        "\n",
        "# Select only the patent data for evaluation\n",
        "true_values = true_values_df[['Year'] + patent_columns.tolist()].set_index('Year')\n",
        "predictions = predictions_df[['Year'] + patent_columns.tolist()].set_index('Year')\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "mae = mean_absolute_error(true_values, predictions)\n",
        "mse = mean_squared_error(true_values, predictions)\n",
        "rmse = mse**0.5\n",
        "r2 = r2_score(true_values, predictions)\n",
        "\n"
      ],
      "metadata": {
        "id": "5vwm6OlmME5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "random forest"
      ],
      "metadata": {
        "id": "Yi8ycbe9MHwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/merged_patent_gdp_data.csv')\n",
        "\n",
        "# Split data\n",
        "train_data = data[data[\"Year\"] <= 2010]\n",
        "test_data = data[data[\"Year\"] > 2010]\n",
        "\n",
        "patent_columns = train_data.columns[1:-5].tolist()\n",
        "economic_columns = [\"rate(%)\", \"GDP\", \"unemployment rate\", \"Per capita disposable income\", \"CCI\"]\n",
        "all_columns = patent_columns + economic_columns\n",
        "\n",
        "train_features_patent = train_data[patent_columns]\n",
        "test_features_patent = test_data[patent_columns]\n",
        "\n",
        "train_features_all = train_data[all_columns]\n",
        "test_features_all = test_data[all_columns]\n",
        "\n",
        "train_labels = train_data[patent_columns]\n",
        "test_labels = test_data[patent_columns]\n",
        "\n",
        "# Post processing function\n",
        "def post_process(predictions):\n",
        "    predictions = np.round(predictions)\n",
        "    predictions = np.maximum(predictions, 0)\n",
        "    return predictions\n",
        "\n",
        "# Train and evaluate with the optimized Random Forest using the best parameters\n",
        "def train_and_evaluate_rf_optimized(train_features, test_features, train_labels, test_labels, best_params):\n",
        "    rf = RandomForestRegressor(**best_params, random_state=42)\n",
        "    rf.fit(train_features, train_labels)\n",
        "\n",
        "    predictions = rf.predict(test_features)\n",
        "    predictions = post_process(predictions)\n",
        "\n",
        "    mse = mean_squared_error(test_labels, predictions)\n",
        "    mae = mean_absolute_error(test_labels, predictions)\n",
        "    r2 = r2_score(test_labels, predictions)\n",
        "\n",
        "    print(\"MSE:\", mse)\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"R^2:\", r2)\n",
        "\n",
        "    return predictions, mse, mae, r2\n",
        "\n",
        "# Random search for hyperparameter optimization\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300, 400],\n",
        "    'max_depth': [None, 10, 15, 20, 25, 30],\n",
        "    'min_samples_split': [2, 5, 10, 15],\n",
        "    'min_samples_leaf': [1, 2, 4, 6]\n",
        "}\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
        "                                   n_iter=20, cv=3, n_jobs=-1, verbose=2, random_state=42, scoring='r2')\n",
        "\n",
        "random_search.fit(train_features_all, train_labels)\n",
        "\n",
        "best_params_random = random_search.best_params_\n",
        "\n",
        "# Train and evaluate models with optimized parameters\n",
        "print(\"Performance metrics for model with patent data only:\")\n",
        "predictions_patent_optimized, _, _, _ = train_and_evaluate_rf_optimized(train_features_patent, test_features_patent, train_labels, test_labels, best_params_random)\n",
        "\n",
        "print(\"\\nPerformance metrics for model with patent and economic data:\")\n",
        "predictions_all_optimized, _, _, _ = train_and_evaluate_rf_optimized(train_features_all, test_features_all, train_labels, test_labels, best_params_random)\n",
        "\n",
        "# Save predictions to CSV files\n",
        "predictions_patent_df = pd.DataFrame(predictions_patent_optimized, columns=patent_columns)\n",
        "predictions_patent_df[\"Year\"] = test_data[\"Year\"].values\n",
        "predictions_patent_df.to_csv(\"/content/patent_only_optimized.csv\", index=False)\n",
        "\n",
        "predictions_all_df = pd.DataFrame(predictions_all_optimized, columns=patent_columns)\n",
        "predictions_all_df[\"Year\"] = test_data[\"Year\"].values\n",
        "predictions_all_df.to_csv(\"/content/patent_and_economic_optimized.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuO5kuRGMMsN",
        "outputId": "a16bd231-c203-4b8c-a234-1573c3d79316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "Performance metrics for model with patent data only:\n",
            "MSE: 0.17642585551330797\n",
            "MAE: 0.13384030418250953\n",
            "R^2: 0.5039720260727867\n",
            "\n",
            "Performance metrics for model with patent and economic data:\n",
            "MSE: 0.15893536121673005\n",
            "MAE: 0.12851711026615972\n",
            "R^2: 0.5325457179069347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/merged_patent_gdp_data.csv')\n",
        "\n",
        "# Split data\n",
        "train_data = data[data[\"Year\"] <= 2010]\n",
        "test_data = data[data[\"Year\"] > 2010]\n",
        "\n",
        "patent_columns = train_data.columns[1:-5].tolist()\n",
        "economic_columns = [\"rate(%)\", \"GDP\", \"unemployment rate\", \"Per capita disposable income\", \"CCI\"]\n",
        "all_columns = patent_columns + economic_columns\n",
        "\n",
        "train_features_patent = train_data[patent_columns]\n",
        "test_features_patent = test_data[patent_columns]\n",
        "\n",
        "train_features_all = train_data[all_columns]\n",
        "test_features_all = test_data[all_columns]\n",
        "\n",
        "train_labels = train_data[patent_columns]\n",
        "test_labels = test_data[patent_columns]\n",
        "\n",
        "# Post processing function\n",
        "def post_process(predictions):\n",
        "    predictions = np.round(predictions)\n",
        "    predictions = np.maximum(predictions, 0)\n",
        "    return predictions\n",
        "\n",
        "# Train and evaluate using Gradient Boosting for the entire dataset\n",
        "def train_and_evaluate_gbm(train_features, test_features, train_labels, test_labels, model_type):\n",
        "    predictions = []\n",
        "    for column in patent_columns:\n",
        "        gbm = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "        gbm.fit(train_features, train_labels[column])\n",
        "        pred = gbm.predict(test_features)\n",
        "        predictions.append(post_process(pred))\n",
        "\n",
        "    predictions = np.array(predictions).T\n",
        "\n",
        "    mse = mean_squared_error(test_labels, predictions)\n",
        "    mae = mean_absolute_error(test_labels, predictions)\n",
        "    r2 = r2_score(test_labels, predictions)\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Train and evaluate models\n",
        "print(\"Performance metrics for GBM model with patent data only:\")\n",
        "predictions_patent_gbm = train_and_evaluate_gbm(train_features_patent, test_features_patent, train_labels, test_labels, \"patent data only\")\n",
        "predictions_patent_df = pd.DataFrame(predictions_patent_gbm, columns=patent_columns)\n",
        "predictions_patent_df[\"Year\"] = test_data[\"Year\"].values\n",
        "predictions_patent_df.to_csv(\"/content/patent_only_gbm.csv\", index=False)\n",
        "\n",
        "print(\"\\nPerformance metrics for GBM model with patent and economic data:\")\n",
        "predictions_all_gbm = train_and_evaluate_gbm(train_features_all, test_features_all, train_labels, test_labels, \"patent and economic data\")\n",
        "predictions_all_df = pd.DataFrame(predictions_all_gbm, columns=patent_columns)\n",
        "predictions_all_df[\"Year\"] = test_data[\"Year\"].values\n",
        "predictions_all_df.to_csv(\"/content/patent_and_economic_gbm.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6dmrhwcMTbI",
        "outputId": "8da86db8-96c4-45cd-8f3d-b04c14bef863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance metrics for GBM model with patent data only:\n",
            "\n",
            "Performance metrics for GBM model with patent and economic data:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Load data\n",
        "merged_data = pd.read_csv('/content/merged_patent_gdp_data.csv')\n",
        "\n",
        "# Filter only patent columns based on the structure of your data\n",
        "patent_columns = merged_data.columns[1:-5].tolist()  # Assuming last 5 columns are economic data\n",
        "\n",
        "# Select only patent related columns\n",
        "patent_data = merged_data[[\"Year\"] + patent_columns]\n",
        "\n",
        "# Melt the data\n",
        "melted_data = patent_data.melt(id_vars=[\"Year\"], var_name=\"Feature\", value_name=\"Value\")\n",
        "melted_data[\"Year\"] = melted_data[\"Year\"].astype(int)\n",
        "\n",
        "# Split the data\n",
        "train_data = melted_data[melted_data[\"Year\"] <= 2010]\n",
        "test_data = melted_data[melted_data[\"Year\"] > 2010]\n",
        "\n",
        "# Add lag feature\n",
        "train_data[\"Lag1\"] = train_data.groupby(\"Feature\")[\"Value\"].shift(1)\n",
        "test_data[\"Lag1\"] = test_data.groupby(\"Feature\")[\"Value\"].shift(1)\n",
        "train_data[\"Lag1\"].fillna(0, inplace=True)\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "kf1Tp-fOMamt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Add rolling mean feature\n",
        "train_data[\"Rolling_Mean_3\"] = train_data.groupby(\"Feature\")[\"Value\"].transform(lambda x: x.rolling(3).mean().shift(1))\n",
        "test_data[\"Rolling_Mean_3\"] = test_data.groupby(\"Feature\")[\"Value\"].transform(lambda x: x.rolling(3).mean().shift(1))\n",
        "train_data[\"Rolling_Mean_3\"].fillna(0, inplace=True)\n",
        "\n",
        "# Label encode the feature names\n",
        "label_encoder = LabelEncoder()\n",
        "train_data[\"Feature_Encoded\"] = label_encoder.fit_transform(train_data[\"Feature\"])\n",
        "test_data[\"Feature_Encoded\"] = label_encoder.transform(test_data[\"Feature\"])\n",
        "\n",
        "# Function to automatically select ARIMA parameters\n",
        "def auto_arima_parameters(train_series, p_max=3, d_max=2, q_max=3):\n",
        "    best_aic = float('inf')\n",
        "    best_order = None\n",
        "\n",
        "    for p in range(p_max + 1):\n",
        "        for d in range(d_max + 1):\n",
        "            for q in range(q_max + 1):\n",
        "                try:\n",
        "                    model = ARIMA(train_series, order=(p,d,q))\n",
        "                    model_fit = model.fit()\n",
        "                    if model_fit.aic < best_aic:\n",
        "                        best_aic = model_fit.aic\n",
        "                        best_order = (p, d, q)\n",
        "                except:\n",
        "                    continue\n",
        "    return best_order\n",
        "\n",
        "# Perform ARIMA modeling and predictions\n",
        "maes = []\n",
        "mses = []\n",
        "r2_scores = []\n",
        "predictions_dict = {}\n",
        "\n",
        "for feature in train_data[\"Feature\"].unique():\n",
        "    train_series = train_data[train_data[\"Feature\"] == feature].set_index(\"Year\")[\"Value\"]\n",
        "    test_series = test_data[test_data[\"Feature\"] == feature].set_index(\"Year\")[\"Value\"]\n",
        "\n",
        "    best_order = auto_arima_parameters(train_series)\n",
        "\n",
        "    model = ARIMA(train_series, order=best_order)\n",
        "    model_fit = model.fit()\n",
        "    predictions = model_fit.forecast(steps=len(test_series))\n",
        "\n",
        "    predictions_dict[feature] = predictions\n",
        "\n",
        "    maes.append(mean_absolute_error(test_series, predictions))\n",
        "    mses.append(mean_squared_error(test_series, predictions))\n",
        "    r2_scores.append(r2_score(test_series, predictions))\n",
        "\n",
        "average_mae = sum(maes) / len(maes)\n",
        "average_mse = sum(mses) / len(mses)\n",
        "average_rmse = (average_mse)**0.5\n",
        "average_r2 = sum(r2_scores) / len(r2_scores)\n",
        "\n",
        "predictions_df = pd.DataFrame(predictions_dict)\n",
        "predictions_filepath = \"/content/patent_predictions_arima.csv\"\n",
        "predictions_df.to_csv(predictions_filepath)\n"
      ],
      "metadata": {
        "id": "r481o0wyMfc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. Load the data\n",
        "merged_data = pd.read_csv('/content/merged_patent_gdp_data.csv')\n",
        "\n",
        "# 2. Data preprocessing\n",
        "# Adjusting the transpose based on the correct column names\n",
        "melted_data = merged_data.melt(id_vars=[\"Year\", \"rate(%)\", \"GDP\", \"unemployment rate\", \"Per capita disposable income\", \"CCI\"],\n",
        "                               var_name=\"Patent_Type\",\n",
        "                               value_name=\"Patent_Count\")\n",
        "\n",
        "# Split the dataset\n",
        "train_data = melted_data[melted_data[\"Year\"] <= 2010]\n",
        "test_data = melted_data[melted_data[\"Year\"] > 2010]\n",
        "\n",
        "# Add lagged terms\n",
        "train_data[\"Lag1\"] = train_data.groupby(\"Patent_Type\")[\"Patent_Count\"].shift(1)\n",
        "test_data[\"Lag1\"] = test_data.groupby(\"Patent_Type\")[\"Patent_Count\"].shift(1)\n",
        "train_data[\"Lag1\"].fillna(0, inplace=True)\n",
        "\n",
        "# Add rolling window statistical features\n",
        "train_data[\"Rolling_Mean_3\"] = train_data.groupby(\"Patent_Type\")[\"Patent_Count\"].transform(lambda x: x.rolling(3).mean().shift(1))\n",
        "test_data[\"Rolling_Mean_3\"] = test_data.groupby(\"Patent_Type\")[\"Patent_Count\"].transform(lambda x: x.rolling(3).mean().shift(1))\n",
        "train_data[\"Rolling_Mean_3\"].fillna(0, inplace=True)\n",
        "\n",
        "# Perform label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "train_data[\"Patent_Type_Encoded\"] = label_encoder.fit_transform(train_data[\"Patent_Type\"])\n",
        "test_data[\"Patent_Type_Encoded\"] = label_encoder.transform(test_data[\"Patent_Type\"])\n",
        "\n",
        "# 3. Predict using the SARIMAX model\n",
        "fixed_order = (1, 1, 1)\n",
        "predictions_dict = {}\n",
        "\n",
        "for patent_type in train_data[\"Patent_Type\"].unique():\n",
        "    train_subset = train_data[train_data[\"Patent_Type\"] == patent_type].set_index(\"Year\")\n",
        "    test_subset = test_data[test_data[\"Patent_Type\"] == patent_type].set_index(\"Year\")\n",
        "\n",
        "    exog_train = train_subset[[\"rate(%)\", \"GDP\", \"unemployment rate\", \"Per capita disposable income\", \"CCI\"]]\n",
        "    exog_test = test_subset[[\"rate(%)\", \"GDP\", \"unemployment rate\", \"Per capita disposable income\", \"CCI\"]]\n",
        "\n",
        "    model = SARIMAX(train_subset[\"Patent_Count\"],\n",
        "                    exog=exog_train,\n",
        "                    order=fixed_order,\n",
        "                    seasonal_order=(0, 0, 0, 0),\n",
        "                    enforce_stationarity=False,\n",
        "                    enforce_invertibility=False)\n",
        "\n",
        "    model_fit = model.fit(disp=False)\n",
        "    predictions = model_fit.forecast(steps=len(test_subset), exog=exog_test)\n",
        "\n",
        "    # Convert the predictions to integers and ensure they are positive\n",
        "    predictions = predictions.round().clip(lower=0)\n",
        "\n",
        "    predictions_dict[patent_type] = predictions\n",
        "\n",
        "# 4. Save the predictions to a CSV file\n",
        "predictions_df = pd.DataFrame(predictions_dict)\n",
        "predictions_df.to_csv(\"SARIMAX_predictions.csv\")\n",
        "\n",
        "print(\"Predictions completed!\")\n"
      ],
      "metadata": {
        "id": "IMMO-WVNMlpl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}